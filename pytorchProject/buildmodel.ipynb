{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:25:37.099428600Z",
     "start_time": "2023-10-18T14:25:34.299587900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from myconfig.Config import *\n",
    "from trans1utils.DataGenerater import *\n",
    "from transformer.Transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "vocab,vocab_size = vocab_config()\n",
    "\n",
    "hla_max_len,pep_max_len,tcr_max_len,hla_pep_concat_len,pep_tcr_concat_len = data_config()\n",
    "\n",
    "d_model, d_ff, d_k, d_v, _, n_heads, epochs, batch_size, threshold, dropout_rate, max_len, device \\\n",
    "    = model_config()\n",
    "\n",
    "seed = run_config()\n",
    "\n",
    "# train_hla_pep_loader,test_hla_pep_loader \\\n",
    "#     = hla_pep_data_loader('hla_pep_dataset',vocab,hla_max_len,pep_max_len,batch_size,0.9)\n",
    "\n",
    "train_pep_tcr_loader,test_pep_tcr_loader \\\n",
    "    = pep_tcr_data_loader('pep_tcr_dataset',vocab,pep_max_len,tcr_max_len,batch_size,0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:25:40.656509500Z",
     "start_time": "2023-10-18T14:25:39.040101500Z"
    }
   },
   "id": "6a10f4461f3a1f17"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_hla_pep_loader,test_hla_pep_loader \\\n",
    "    = hla_pep_data_loader('independent_set',vocab,hla_max_len,pep_max_len,batch_size,0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:19:28.382105500Z",
     "start_time": "2023-10-18T14:19:26.081073600Z"
    }
   },
   "id": "a2327b7856c72bc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n_layers in range(1,10):\n",
    "    for n_heads in range(1,6):\n",
    "        for fold in range(5):\n",
    "            \n",
    "            train_loader = data_with_loader(vocab,'train',fold,hla_max_len,pep_max_len,batch_size)\n",
    "            val_loader = data_with_loader(vocab,'val',fold,hla_max_len,pep_max_len,batch_size)\n",
    "\n",
    "            model = Transformer(vocab_size,n_enc_layers=n_layers,n_enc_heads=n_heads,n_dec_layers=n_layers,n_dec_heads=n_heads,hla_pep_concat_len=hla_pep_concat_len,pep_tcr_concat_len=pep_tcr_concat_len).to(device)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "            dir_saver = './model/'\n",
    "            path_saver = './model/model_hp_layer{}_head{}_fold{}.pth'.format(n_layers, n_heads,fold)\n",
    "\n",
    "            metric_best, ep_best,time_train = 0, -1, 0\n",
    "            for epoch in range(1,epochs+1):\n",
    "                _,_,_ \\\n",
    "                    = model.train_per_epoch(train_loader,epoch,epochs,criterion,optimizer,num_group=1,threshold=threshold,metrics_print_=True)\n",
    "\n",
    "                _, _, metrics_val \\\n",
    "                    = model.eval_per_epoch(val_loader, epoch, epochs,criterion,num_group=1,threshold=threshold,metrics_print_=True)\n",
    "\n",
    "                metrics_ep_avg = sum(metrics_val[:4])/4\n",
    "                \n",
    "                if metrics_ep_avg > metric_best:\n",
    "                    metric_best, ep_best = metrics_ep_avg, epoch\n",
    "                    if not os.path.exists(dir_saver):\n",
    "                        os.makedirs(dir_saver)\n",
    "                    print('Best epoch = {} | Best metrics_ep_avg = {:.4f}--------'.format(ep_best, metric_best))\n",
    "                    print('Saving model Path saver: {} --------'.format(path_saver))\n",
    "                    torch.save(model.state_dict(), path_saver)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4073425feeea84e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=5)\n",
    "# for n_layers in range(1,10):\n",
    "#     for n_heads in range(1,6):\n",
    "#         for fold,(train_index,val_index) in enumerate(kf.split(train_pep_tcr_loader.dataset)):\n",
    "# \n",
    "#             train_pep_tcr_set = train_pep_tcr_loader.dataset[train_index]\n",
    "#             val_pep_tcr_set = train_pep_tcr_loader.dataset[val_index]\n",
    "# \n",
    "#             train_pep_tcr \\\n",
    "#                 = Data.DataLoader(PEP_TCR_DataSet(train_pep_tcr_set[0], train_pep_tcr_set[1],train_pep_tcr_set[2]), batch_size,shuffle=False, num_workers=0)\n",
    "#             val_pep_tcr \\\n",
    "#                 = Data.DataLoader(PEP_TCR_DataSet(val_pep_tcr_set[0], val_pep_tcr_set[1],    val_pep_tcr_set[2]), batch_size,shuffle=False, num_workers=0)\n",
    "# \n",
    "#             model \\\n",
    "#                 = Transformer(vocab_size,n_enc_layers=n_layers,n_enc_heads=n_heads,n_dec_layers=n_layers,n_dec_heads=n_heads,hla_pep_concat_len=hla_pep_concat_len,pep_tcr_concat_len=pep_tcr_concat_len).to(device)\n",
    "# \n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "#             optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "# \n",
    "#             dir_saver = './model/'\n",
    "#             path_saver = './model/model_pt_layer{}_head{}_fold{}.pth'.format(n_layers, n_heads,fold)\n",
    "# \n",
    "#             metric_best, ep_best,time_train = 0, -1, 0\n",
    "#             for epoch in range(1,epochs+1):\n",
    "#                 _,_,_ \\\n",
    "#                     = model.train_per_epoch(train_pep_tcr,epoch,epochs,criterion,optimizer,num_group=2,threshold=threshold,metrics_print_=True)\n",
    "# \n",
    "#                 _, _, metrics_val \\\n",
    "#                     = model.eval_per_epoch(val_pep_tcr, epoch, epochs,criterion,num_group=2,threshold=threshold,metrics_print_=True)\n",
    "# \n",
    "# \n",
    "#                 metrics_ep_avg = metrics_val[0] # auc\n",
    "# \n",
    "#                 if metrics_ep_avg > metric_best:\n",
    "#                     metric_best, ep_best = metrics_ep_avg, epoch\n",
    "#                     if not os.path.exists(dir_saver):\n",
    "#                         os.makedirs(dir_saver)\n",
    "#                     print('Best epoch = {} | Best metrics_ep_avg = {:.4f}--------'.format(ep_best, metric_best))\n",
    "#                     print('Saving model Path saver: {} --------'.format(path_saver))\n",
    "#                     torch.save(model.state_dict(), path_saver)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc36e17f7150c161"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size,n_enc_layers=9,n_enc_heads=5,n_dec_layers=9,n_dec_heads=5,hla_pep_concat_len=hla_pep_concat_len,pep_tcr_concat_len=pep_tcr_concat_len,device=device).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:56:56.360377100Z",
     "start_time": "2023-10-18T13:56:55.936071200Z"
    }
   },
   "id": "5a8cb0d28fa540b9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Transformer(\n  (hla_embedding): Embedding(\n    (src_emb): Embedding(27, 64)\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (pep_embedding): Embedding(\n    (src_emb): Embedding(27, 64)\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (tcr_embedding): Embedding(\n    (src_emb): Embedding(27, 64)\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (hla_encoder): Encoder(\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (pep_encoder): Encoder(\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (tcr_encoder): Encoder(\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (hla_pep_decoder): Decoder(\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (pep_tcr_decoder): Decoder(\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (hla_pep_projection): Sequential(\n    (0): Linear(in_features=3136, out_features=256, bias=True)\n    (1): ReLU(inplace=True)\n    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Linear(in_features=256, out_features=64, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Linear(in_features=64, out_features=2, bias=True)\n  )\n  (pep_tcr_projection): Sequential(\n    (0): Linear(in_features=3136, out_features=256, bias=True)\n    (1): ReLU(inplace=True)\n    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Linear(in_features=256, out_features=64, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Linear(in_features=64, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:57:01.630353500Z",
     "start_time": "2023-10-18T13:57:01.536595800Z"
    }
   },
   "id": "714ac192cdfb5133"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mload_state_dict(\n\u001B[0;32m      2\u001B[0m     torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mProjectsSTC\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mpytorchProject\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mmodel_pt_layer9_head5_fold4.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load('D:\\ProjectsSTC\\pytorchProject\\model\\model_pt_layer9_head5_fold4.pth')\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:25:52.905679300Z",
     "start_time": "2023-10-18T14:25:52.811918500Z"
    }
   },
   "id": "407f37d4af41a72b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39meval_per_epoch(train_pep_tcr_loader,\u001B[38;5;28;01mNone\u001B[39;00m,\u001B[38;5;28;01mNone\u001B[39;00m,criterion,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m0.5\u001B[39m,\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model.eval_per_epoch(train_pep_tcr_loader,None,None,criterion,2,0.5,True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:25:49.431401Z",
     "start_time": "2023-10-18T14:25:47.935920400Z"
    }
   },
   "id": "c9a37b20e4b01e77"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******开始验证******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:05<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是评估得分:\n",
      "MCC Error:  921653598363345\n",
      "y_true: 0 = 7691 | 1 = 3841\n",
      "y_pred: 0 = 7197 | 1 = 4335\n",
      "tn = 7053, fp = 638, fn = 144, tp = 3697\n",
      "auc=0.9657|sensitivity=0.9625|specificity=0.9170|acc=0.9322|mcc=nan\n",
      "precision=0.8528|recall=0.9625|f1=0.9044|ap=0.8913\n",
      "******结束验证: Loss = 0.191216******\n"
     ]
    },
    {
     "data": {
      "text/plain": "(([0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   0,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   ...],\n  array([0, 0, 1, ..., 0, 0, 0]),\n  [9.611275e-07,\n   4.913914e-09,\n   0.93260217,\n   1.4672402e-06,\n   6.32494e-05,\n   0.8549641,\n   0.0011846833,\n   9.906785e-07,\n   0.5945316,\n   1.8418705e-06,\n   0.011616415,\n   0.88120055,\n   0.7513061,\n   0.008922956,\n   1.0158055e-05,\n   0.00090843654,\n   0.001448792,\n   1.2142743e-06,\n   0.76543254,\n   0.8596208,\n   0.93322724,\n   0.0011828082,\n   0.00014957917,\n   0.51987296,\n   0.49065498,\n   0.12923692,\n   3.952745e-06,\n   0.09006609,\n   7.9160986e-07,\n   0.85493356,\n   0.0014175397,\n   0.9441155,\n   1.033356e-06,\n   0.3623844,\n   3.3796476e-07,\n   0.90427494,\n   3.7627188e-07,\n   9.955902e-06,\n   0.0034498952,\n   6.942274e-08,\n   0.000638606,\n   0.00046013526,\n   9.918866e-07,\n   0.9238657,\n   0.8661388,\n   0.0001562755,\n   0.004520964,\n   9.225515e-05,\n   6.2834944e-05,\n   0.88837266,\n   0.2599961,\n   0.86884284,\n   0.9324958,\n   7.023491e-05,\n   3.3329434e-05,\n   3.4705572e-05,\n   0.9029083,\n   0.00027792336,\n   0.29545185,\n   8.74782e-07,\n   0.8963935,\n   0.001619332,\n   0.8840805,\n   0.00013082904,\n   0.8659318,\n   6.623822e-06,\n   0.00045950455,\n   0.82104784,\n   9.347839e-08,\n   1.5474956e-05,\n   4.140112e-07,\n   0.00084126723,\n   0.86194605,\n   0.0015462502,\n   0.0032739455,\n   0.07930455,\n   0.9265643,\n   3.291039e-09,\n   1.7748194e-05,\n   0.8997895,\n   0.8731433,\n   1.0068227e-05,\n   2.4925863e-05,\n   0.92678875,\n   0.9682652,\n   0.91138214,\n   8.260598e-09,\n   0.89571935,\n   1.08249196e-07,\n   0.93562436,\n   5.468124e-06,\n   0.00018524288,\n   0.00019107242,\n   2.8281288e-08,\n   0.80966985,\n   0.22591183,\n   0.029410502,\n   1.7862145e-05,\n   6.628283e-05,\n   0.004176104,\n   0.84968156,\n   0.9032062,\n   3.4850384e-06,\n   1.7329471e-08,\n   0.88174313,\n   3.8097074e-05,\n   0.0052775503,\n   0.79607034,\n   0.7684932,\n   0.9262451,\n   9.4449945e-08,\n   4.095777e-06,\n   0.6898159,\n   0.9029749,\n   0.00029613098,\n   0.9231449,\n   0.9257775,\n   2.1039563e-05,\n   0.83717865,\n   1.1395603e-06,\n   8.9609665e-07,\n   0.89225036,\n   0.89142257,\n   0.9347433,\n   2.3751834e-05,\n   1.7832295e-05,\n   0.0037702788,\n   5.274821e-05,\n   2.0985216e-09,\n   0.5817967,\n   2.7551885e-06,\n   0.0001508879,\n   0.0035721194,\n   0.8742546,\n   8.400958e-05,\n   1.9809487e-09,\n   0.02459606,\n   0.89147544,\n   0.0013010821,\n   0.9685768,\n   0.0002475213,\n   0.00018443236,\n   0.76626694,\n   0.6425848,\n   0.9498639,\n   0.877252,\n   0.0022277662,\n   0.9399191,\n   0.0061160238,\n   0.80981636,\n   2.9711284e-05,\n   7.1789054e-06,\n   8.588243e-08,\n   3.179283e-06,\n   0.034154717,\n   2.3026001e-05,\n   0.93255246,\n   0.8804297,\n   0.79549414,\n   1.0222037e-05,\n   0.0041741338,\n   0.9115308,\n   0.5627647,\n   0.933645,\n   0.00013322779,\n   2.0974317e-06,\n   0.00030483727,\n   0.0025367397,\n   0.9162209,\n   0.8997872,\n   0.00017721798,\n   0.00022571895,\n   0.06532161,\n   4.95331e-05,\n   0.023609,\n   0.89890796,\n   0.0010764059,\n   0.0028225726,\n   0.029476065,\n   0.0026201224,\n   3.2166014e-07,\n   0.0031750696,\n   3.924795e-06,\n   3.811466e-05,\n   0.9560804,\n   5.6399156e-05,\n   8.61635e-06,\n   0.00033881434,\n   0.009756105,\n   2.5547066e-05,\n   1.6456294e-08,\n   0.8626991,\n   0.0017882781,\n   0.8557251,\n   0.00031743856,\n   0.89071804,\n   0.9119084,\n   0.012129496,\n   0.9351801,\n   1.5323534e-06,\n   0.723202,\n   0.0012081872,\n   0.8711927,\n   0.00045823294,\n   1.1387541e-06,\n   0.72131276,\n   0.9515811,\n   2.1298918e-07,\n   0.9231472,\n   0.93665886,\n   0.22908656,\n   3.5055652e-06,\n   0.618597,\n   0.9142366,\n   0.90281,\n   0.8251131,\n   6.4987755e-05,\n   0.90579927,\n   0.40958795,\n   6.856304e-07,\n   1.4068353e-08,\n   6.18789e-05,\n   1.4154341e-06,\n   0.9376367,\n   0.9317986,\n   1.4637316e-05,\n   0.88108873,\n   0.748407,\n   0.0002559742,\n   0.7114406,\n   5.2312804e-10,\n   0.8848926,\n   0.022914885,\n   0.87846607,\n   6.87878e-05,\n   0.8345577,\n   0.2061773,\n   6.568524e-09,\n   3.7773341e-07,\n   0.909069,\n   0.9014245,\n   9.420101e-07,\n   0.87967443,\n   0.92478824,\n   9.813674e-05,\n   0.9027586,\n   7.539466e-05,\n   0.887554,\n   3.058953e-05,\n   0.9277418,\n   2.4432947e-10,\n   0.55591065,\n   0.07801168,\n   1.4601597e-05,\n   0.034462348,\n   1.3938326e-06,\n   0.030558184,\n   0.0069923736,\n   0.030099599,\n   7.387275e-09,\n   0.00093633187,\n   0.8887579,\n   0.00022840519,\n   1.564722e-05,\n   2.3757946e-06,\n   0.0024935827,\n   2.070311e-05,\n   4.5832698e-05,\n   0.7350478,\n   0.8588485,\n   1.1315863e-05,\n   8.8690605e-05,\n   0.90393925,\n   0.9376054,\n   0.90822977,\n   0.005511965,\n   0.8631991,\n   0.00015825745,\n   0.8169878,\n   0.0057355193,\n   0.002767654,\n   0.6949492,\n   0.0012009038,\n   0.9471531,\n   0.002045351,\n   0.001865768,\n   1.1862227e-06,\n   0.8975884,\n   0.9171102,\n   2.4797234e-06,\n   0.9242816,\n   3.3838703e-06,\n   0.9153338,\n   0.40935045,\n   0.0005069913,\n   0.9163379,\n   5.5853816e-06,\n   3.1004794e-09,\n   0.040499747,\n   0.00016971955,\n   0.92137593,\n   0.9643248,\n   0.82445014,\n   0.15866658,\n   0.9270014,\n   1.0128939e-06,\n   0.6330869,\n   5.3148397e-06,\n   0.00012271425,\n   0.004650402,\n   0.83988166,\n   0.00010258207,\n   0.9743456,\n   4.09313e-06,\n   0.0065356265,\n   3.896168e-05,\n   0.6533827,\n   0.00014018093,\n   0.22473817,\n   0.7560821,\n   0.010817296,\n   0.7934567,\n   0.8110182,\n   0.0019619006,\n   0.81907123,\n   0.0005905314,\n   0.00040198257,\n   0.7591286,\n   0.00082308374,\n   0.007182524,\n   0.9386616,\n   4.766303e-06,\n   0.07833511,\n   1.7690698e-07,\n   0.8293085,\n   0.95270723,\n   0.94388777,\n   0.45205742,\n   0.96443796,\n   1.8796803e-07,\n   0.8743108,\n   0.9148965,\n   0.86318725,\n   0.0014075206,\n   0.9559043,\n   4.016083e-06,\n   0.81503654,\n   0.89485854,\n   0.0012177688,\n   3.2383792e-05,\n   0.0084947,\n   9.778743e-06,\n   1.0006705e-05,\n   6.3294e-06,\n   5.082073e-11,\n   0.8512938,\n   0.9534852,\n   1.1334739e-07,\n   0.00011755354,\n   0.010907265,\n   0.818382,\n   0.86295944,\n   0.0023541332,\n   0.808036,\n   0.9066366,\n   3.105747e-09,\n   0.001643587,\n   0.0045558927,\n   4.9556154e-05,\n   0.00022253834,\n   0.14392667,\n   2.1533722e-06,\n   0.8063244,\n   3.2117276e-07,\n   0.0028948754,\n   0.90368056,\n   0.00062780496,\n   6.9385266e-07,\n   0.0087278625,\n   0.0011827237,\n   0.0046990123,\n   0.94466585,\n   0.0064089843,\n   0.8750781,\n   0.3395497,\n   0.78323644,\n   0.0010820667,\n   0.9138266,\n   0.83270645,\n   0.015676314,\n   1.8887099e-09,\n   0.93057626,\n   0.8938832,\n   7.3771706e-05,\n   0.0002013291,\n   0.8839448,\n   6.8782945e-05,\n   1.355391e-06,\n   4.6415138e-05,\n   0.0045623393,\n   0.00034486075,\n   0.89236355,\n   0.00014115244,\n   1.4562882e-05,\n   0.881469,\n   0.9180509,\n   0.9178868,\n   5.2138086e-05,\n   0.0007066807,\n   3.372311e-06,\n   0.0016916565,\n   0.1976129,\n   0.0016757837,\n   0.8985898,\n   1.4989258e-08,\n   0.90119696,\n   4.377876e-06,\n   8.316534e-07,\n   0.9406892,\n   0.8791387,\n   5.442185e-06,\n   0.8209862,\n   0.0073417523,\n   4.753574e-06,\n   9.406818e-05,\n   0.00016586314,\n   0.8603452,\n   5.2553255e-06,\n   0.0008527145,\n   9.204534e-06,\n   0.8864211,\n   0.062365104,\n   0.7730367,\n   0.5218891,\n   0.02030146,\n   5.2119793e-07,\n   0.0039216424,\n   0.9153972,\n   0.16972636,\n   2.6699138e-06,\n   9.4005314e-05,\n   4.536394e-05,\n   0.82164264,\n   0.941835,\n   0.8787983,\n   0.00021348741,\n   0.9162122,\n   0.07219428,\n   0.003005786,\n   0.005529034,\n   0.00051479804,\n   0.022153622,\n   3.6045506e-06,\n   2.0092446e-05,\n   0.8874876,\n   0.58181316,\n   0.00039343492,\n   0.010021019,\n   2.2656133e-09,\n   0.0002393815,\n   0.38117677,\n   0.90652245,\n   0.9243707,\n   0.89589244,\n   0.004386862,\n   0.00657379,\n   3.528148e-05,\n   0.00017461274,\n   0.9238145,\n   0.23381077,\n   0.9086318,\n   0.0008791131,\n   7.977717e-05,\n   1.8091614e-05,\n   0.0016627879,\n   3.1458835e-06,\n   0.03775279,\n   3.381904e-05,\n   0.9441488,\n   2.513408e-06,\n   0.9446672,\n   4.005248e-07,\n   0.88371944,\n   0.000617153,\n   3.4768942e-07,\n   5.7374193e-05,\n   0.81551474,\n   0.9426681,\n   0.81109446,\n   0.8537488,\n   0.8298292,\n   2.256282e-06,\n   0.9039462,\n   0.9194912,\n   0.8205314,\n   0.00086799666,\n   7.877764e-09,\n   4.0331165e-06,\n   0.027179278,\n   0.00068903674,\n   0.002144467,\n   2.6348695e-05,\n   0.8665492,\n   2.777807e-06,\n   0.9011069,\n   0.9220516,\n   0.8923091,\n   1.0610971e-11,\n   8.7191824e-05,\n   0.90067995,\n   0.025307698,\n   0.9357736,\n   0.8924762,\n   0.90495646,\n   0.15072937,\n   0.7831022,\n   0.005397761,\n   0.002508331,\n   6.1473474e-07,\n   0.020061884,\n   0.8463536,\n   4.2842453e-07,\n   0.83579475,\n   0.6947783,\n   0.88614345,\n   0.88327485,\n   0.8630749,\n   0.8484468,\n   7.2699336e-08,\n   0.00013146682,\n   0.94698393,\n   0.8939904,\n   0.9114123,\n   1.1286508e-08,\n   2.1164815e-05,\n   1.9453435e-05,\n   0.23990834,\n   0.0037500192,\n   0.8673417,\n   0.8804141,\n   0.8977689,\n   1.8120105e-05,\n   5.699895e-08,\n   2.9593572e-08,\n   0.13292032,\n   0.8342254,\n   0.000980443,\n   0.5436089,\n   0.00011496342,\n   4.4243883e-05,\n   0.9601849,\n   1.03391085e-05,\n   1.023506e-06,\n   2.141737e-06,\n   0.058752358,\n   0.00018259454,\n   2.0087289e-07,\n   2.8410236e-08,\n   0.008813908,\n   0.867136,\n   0.9876633,\n   4.0171653e-09,\n   0.020238487,\n   4.6704786e-06,\n   0.9704476,\n   0.86814183,\n   0.8581234,\n   0.9226998,\n   5.33766e-08,\n   1.1572107e-05,\n   0.9275667,\n   0.93419605,\n   6.284561e-05,\n   0.8502624,\n   0.9605809,\n   0.9457732,\n   0.9110621,\n   0.8610673,\n   6.0567186e-07,\n   4.343445e-05,\n   0.91490185,\n   8.502363e-07,\n   2.5602623e-05,\n   0.00018062636,\n   3.137339e-06,\n   0.89018625,\n   1.6593832e-06,\n   0.87169176,\n   0.0010138188,\n   2.9278212e-06,\n   0.8922468,\n   0.0017801988,\n   2.2958571e-07,\n   4.1290773e-06,\n   7.6072433e-07,\n   7.6958e-06,\n   0.00026284307,\n   3.5137364e-05,\n   0.93924767,\n   0.87665707,\n   9.621247e-06,\n   2.7166698e-06,\n   8.8627345e-09,\n   0.001021766,\n   0.007225283,\n   7.190731e-09,\n   0.88894975,\n   6.822532e-05,\n   0.0006286321,\n   0.3790427,\n   0.0007656118,\n   1.907797e-06,\n   0.0001326156,\n   4.148628e-06,\n   0.026220823,\n   0.8157495,\n   4.819604e-05,\n   0.8789903,\n   0.93543273,\n   3.5075809e-07,\n   0.012763068,\n   0.11488711,\n   0.0006290568,\n   0.00010417209,\n   0.91442764,\n   2.1517094e-06,\n   0.0023486342,\n   6.391032e-09,\n   0.943786,\n   1.3652707e-05,\n   6.0409373e-05,\n   1.0791624e-05,\n   0.9719496,\n   3.5237565e-05,\n   0.04544292,\n   2.1254296e-05,\n   0.9370092,\n   0.84278625,\n   4.790009e-05,\n   0.9063917,\n   0.8765267,\n   5.0713395e-05,\n   4.478535e-07,\n   0.91951376,\n   0.8746324,\n   0.054967254,\n   0.00014817365,\n   3.9593174e-06,\n   0.00016119346,\n   0.0010000122,\n   0.79348475,\n   0.72573376,\n   0.6873707,\n   0.0003198747,\n   0.00016362939,\n   0.989929,\n   0.00092227326,\n   0.00056002446,\n   6.488436e-08,\n   0.024845956,\n   2.671002e-05,\n   0.9303219,\n   7.317434e-05,\n   0.9107977,\n   0.61551374,\n   1.308063e-06,\n   6.579867e-05,\n   1.1325213e-06,\n   0.81681335,\n   0.7046127,\n   0.928376,\n   0.7866499,\n   6.5256645e-05,\n   9.208174e-05,\n   0.0072487365,\n   0.0006291845,\n   0.0062860404,\n   8.666931e-07,\n   0.994072,\n   0.95411664,\n   0.62774533,\n   0.91999537,\n   1.6509983e-05,\n   9.197005e-06,\n   0.8672931,\n   0.05572109,\n   0.00038316118,\n   0.8551949,\n   0.8808517,\n   5.380352e-06,\n   0.9238727,\n   4.85752e-08,\n   4.2425295e-08,\n   0.90459025,\n   6.031196e-07,\n   1.3929707e-05,\n   2.1748903e-10,\n   4.9058915e-05,\n   9.320297e-05,\n   2.850403e-06,\n   1.1020263e-06,\n   1.6202258e-10,\n   0.00014193711,\n   0.9278086,\n   5.499513e-06,\n   1.337425e-05,\n   9.830498e-06,\n   1.7635648e-07,\n   4.1359293e-05,\n   0.00019002159,\n   0.051672447,\n   0.009611394,\n   4.4578e-07,\n   4.0173454e-05,\n   2.5445247e-06,\n   1.1581182e-05,\n   0.8113233,\n   3.6282072e-05,\n   0.9029368,\n   0.36548045,\n   0.8518676,\n   0.008892827,\n   0.90887517,\n   9.644273e-09,\n   0.84465444,\n   0.8051642,\n   0.8152196,\n   0.9501052,\n   3.562087e-07,\n   0.00051288324,\n   4.596798e-06,\n   0.63887286,\n   8.949353e-08,\n   8.490079e-07,\n   2.900365e-07,\n   4.063854e-05,\n   5.8183196e-05,\n   0.8628612,\n   2.856334e-05,\n   2.9358203e-05,\n   0.0013298688,\n   0.85199344,\n   0.82426125,\n   3.38446e-07,\n   0.017285673,\n   1.1015072e-06,\n   7.9195067e-10,\n   0.70793635,\n   1.25541455e-05,\n   0.0057484363,\n   5.2608837e-12,\n   0.0027270552,\n   0.9120285,\n   0.93772477,\n   0.83835435,\n   0.000119344535,\n   0.9474019,\n   0.8282242,\n   1.1044535e-05,\n   0.0019331017,\n   5.9665806e-09,\n   0.94393736,\n   0.87777174,\n   4.6643516e-05,\n   4.0324894e-06,\n   0.90943694,\n   0.046015687,\n   0.0037542705,\n   0.019281683,\n   7.7322674e-08,\n   0.008878021,\n   0.62655413,\n   2.5708653e-06,\n   0.88389844,\n   0.8830676,\n   0.94647104,\n   0.8187345,\n   0.88982254,\n   0.0011707797,\n   0.00018489543,\n   0.90176153,\n   0.0040690033,\n   0.8590536,\n   1.3724236e-06,\n   0.9146206,\n   9.976103e-05,\n   0.9225076,\n   0.012331297,\n   2.1915591e-06,\n   0.923439,\n   4.9089277e-10,\n   3.0307095e-08,\n   0.006837389,\n   2.1661176e-06,\n   0.0024266497,\n   8.520159e-05,\n   2.1176877e-06,\n   0.000102607504,\n   0.9414097,\n   0.96235424,\n   3.0583842e-09,\n   0.023876067,\n   0.93839717,\n   0.000446935,\n   0.9571795,\n   7.874246e-05,\n   0.0022861788,\n   1.5169323e-06,\n   1.7156244e-05,\n   0.6339017,\n   0.8204445,\n   0.9348731,\n   5.2467897e-07,\n   0.8809112,\n   6.796875e-05,\n   2.3464936e-05,\n   0.00085002446,\n   0.88108987,\n   0.87867874,\n   0.067629926,\n   7.424986e-05,\n   0.8447673,\n   0.97089124,\n   0.9538742,\n   0.91848814,\n   1.0180411e-05,\n   2.470539e-06,\n   0.02289607,\n   0.0045990827,\n   0.77274895,\n   0.9035064,\n   0.0047784843,\n   0.16249694,\n   0.03477017,\n   3.1449803e-05,\n   2.5231345e-06,\n   0.7829457,\n   0.10078736,\n   4.2047315e-08,\n   9.262342e-06,\n   0.3847858,\n   0.36886942,\n   0.008720588,\n   1.0269678e-09,\n   0.008668829,\n   1.4201855e-06,\n   0.009623394,\n   0.8931981,\n   0.82061565,\n   0.019680893,\n   0.009294479,\n   9.3411e-05,\n   3.7091736e-06,\n   4.6153031e-10,\n   0.034978006,\n   0.17459856,\n   4.914901e-05,\n   4.2926214e-08,\n   1.7356486e-05,\n   1.3049166e-07,\n   0.0048395135,\n   6.683086e-06,\n   4.306236e-06,\n   1.1517937e-05,\n   2.011977e-05,\n   0.94743,\n   0.8823941,\n   0.8908183,\n   5.4672194e-07,\n   0.0027455257,\n   0.94973063,\n   0.95756024,\n   8.880666e-06,\n   2.5694074e-09,\n   2.4959592e-05,\n   0.005934933,\n   0.00085315824,\n   0.00020354378,\n   6.0155884e-07,\n   0.8695702,\n   1.41879e-05,\n   0.870312,\n   1.8801204e-05,\n   0.2210323,\n   0.05505251,\n   0.93715686,\n   0.00016046471,\n   9.2065595e-05,\n   3.7547146e-05,\n   0.92780787,\n   0.039361537,\n   6.014784e-06,\n   2.103432e-07,\n   0.9315221,\n   8.708258e-07,\n   1.3240337e-06,\n   0.7158188,\n   6.6183544e-05,\n   0.8791478,\n   4.1943247e-05,\n   0.25896415,\n   9.123932e-08,\n   0.6976715,\n   0.9160549,\n   0.9441781,\n   7.4534415e-05,\n   1.0188057e-07,\n   0.00036961833,\n   9.9492936e-05,\n   0.00096403685,\n   0.0009917454,\n   0.0001385546,\n   0.024316952,\n   2.9129409e-05,\n   0.86534727,\n   6.518579e-07,\n   5.9061546e-07,\n   0.7870842,\n   0.91154146,\n   0.92721033,\n   0.07632621,\n   0.00034564576,\n   0.0066532977,\n   7.1780223e-06,\n   0.0070150667,\n   0.8921628,\n   0.0011544664,\n   3.9166676e-05,\n   1.8709965e-05,\n   2.1260532e-06,\n   0.00011747711,\n   0.94138,\n   0.78775114,\n   0.918135,\n   2.998974e-07,\n   0.9360254,\n   4.7834354e-05,\n   0.47435614,\n   0.92584854,\n   0.000442029,\n   3.4668685e-07,\n   0.004504812,\n   0.89941394,\n   0.9320305,\n   0.8034508,\n   0.045220934,\n   0.8996771,\n   0.86922556,\n   0.0003979376,\n   6.068443e-08,\n   0.9250599,\n   0.059344053,\n   0.89195466,\n   0.73599595,\n   0.640001,\n   0.005029082,\n   0.0025090054,\n   0.8977719,\n   0.59825534,\n   0.02083442,\n   9.325063e-05,\n   0.75097466,\n   0.3719718,\n   7.5755874e-05,\n   6.502288e-06,\n   0.2599299,\n   0.00019436062,\n   1.2236097e-07,\n   0.9265265,\n   0.00040983091,\n   0.33754432,\n   0.06748908,\n   1.3456563e-05,\n   0.9382365,\n   0.8865812,\n   0.0052006254,\n   0.0023170589,\n   0.002457254,\n   0.7664061,\n   0.718656,\n   3.779283e-05,\n   1.5784488e-05,\n   0.0016194507,\n   4.7664985e-06,\n   0.890348,\n   0.7459664,\n   0.015673947,\n   0.93607813,\n   0.91321325,\n   0.945509,\n   0.0020117844,\n   3.3701058e-09,\n   9.1399204e-05,\n   0.4590951,\n   0.0063147657,\n   0.00013262268,\n   8.5345977e-07,\n   0.00029255528,\n   0.8685287,\n   2.48693e-06,\n   ...]),\n [tensor(0.1971, device='cuda:0'),\n  tensor(0.2128, device='cuda:0'),\n  tensor(0.1984, device='cuda:0'),\n  tensor(0.1711, device='cuda:0'),\n  tensor(0.1818, device='cuda:0'),\n  tensor(0.1733, device='cuda:0'),\n  tensor(0.1622, device='cuda:0'),\n  tensor(0.2122, device='cuda:0'),\n  tensor(0.1988, device='cuda:0'),\n  tensor(0.1711, device='cuda:0'),\n  tensor(0.2299, device='cuda:0'),\n  tensor(0.1858, device='cuda:0')],\n (0.9656567143620872,\n  0.932188692334374,\n  nan,\n  0.9043542074363993,\n  0.9625097630825306,\n  0.9170458978026265,\n  0.8528258362168397,\n  0.9625097630825306,\n  0.8913289971443354))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval_per_epoch(test_pep_tcr_loader,None,None,criterion,2,0.5,True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:04:23.987271700Z",
     "start_time": "2023-10-18T14:04:18.154020Z"
    }
   },
   "id": "58501cf381652dea"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load('D:\\ProjectsSTC\\pytorchProject\\model\\model_hp_layer9_head5_fold4.pth')\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:20:45.826700400Z",
     "start_time": "2023-10-18T14:20:45.584001800Z"
    }
   },
   "id": "fe3e10616bc464f2"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Transformer(\n  (hla_embedding): Embedding(\n    (src_emb): Embedding(27, 64)\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (pep_embedding): Embedding(\n    (src_emb): Embedding(27, 64)\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (tcr_embedding): Embedding(\n    (src_emb): Embedding(27, 64)\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (hla_encoder): Encoder(\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (pep_encoder): Encoder(\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (tcr_encoder): Encoder(\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): EncoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (hla_pep_decoder): Decoder(\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (pep_tcr_decoder): Decoder(\n    (pos_emb): PositionEmbedding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (1): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (2): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (3): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (4): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (5): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (6): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (7): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n      (8): DecoderLayer(\n        (multi_head_attention): MultiHeadAttention(\n          (W_Q): Linear(in_features=64, out_features=320, bias=False)\n          (W_K): Linear(in_features=64, out_features=320, bias=False)\n          (W_V): Linear(in_features=64, out_features=320, bias=False)\n          (FC): Linear(in_features=320, out_features=64, bias=False)\n        )\n        (pos_wise_feed_forward_net): PosWiseFeedForwardNet(\n          (FC): Sequential(\n            (0): Linear(in_features=64, out_features=512, bias=False)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=64, bias=False)\n          )\n        )\n      )\n    )\n  )\n  (hla_pep_projection): Sequential(\n    (0): Linear(in_features=3136, out_features=256, bias=True)\n    (1): ReLU(inplace=True)\n    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Linear(in_features=256, out_features=64, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Linear(in_features=64, out_features=2, bias=True)\n  )\n  (pep_tcr_projection): Sequential(\n    (0): Linear(in_features=3136, out_features=256, bias=True)\n    (1): ReLU(inplace=True)\n    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Linear(in_features=256, out_features=64, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Linear(in_features=64, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:20:48.420710600Z",
     "start_time": "2023-10-18T14:20:48.326934900Z"
    }
   },
   "id": "54610464da69f3ee"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******开始验证******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [01:12<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是评估得分:\n",
      "MCC Error:  35395917891480367104\n",
      "y_true: 0 = 76918 | 1 = 77376\n",
      "y_pred: 0 = 75056 | 1 = 79238\n",
      "tn = 69431, fp = 7487, fn = 5625, tp = 71751\n",
      "auc=0.9705|sensitivity=0.9273|specificity=0.9027|acc=0.9150|mcc=nan\n",
      "precision=0.9055|recall=0.9273|f1=0.9163|ap=0.9688\n",
      "******结束验证: Loss = 0.219511******\n"
     ]
    },
    {
     "data": {
      "text/plain": "(([1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   ...],\n  array([1, 1, 1, ..., 1, 1, 1]),\n  [0.9891916,\n   0.9980615,\n   0.9992173,\n   0.9997936,\n   0.6207838,\n   0.52556586,\n   0.7266965,\n   0.9978846,\n   0.8562572,\n   0.99721515,\n   0.9762864,\n   0.9783759,\n   0.9950094,\n   0.9993925,\n   0.9944613,\n   0.97606874,\n   0.99603325,\n   0.29661542,\n   0.99939346,\n   0.99878114,\n   0.9992034,\n   0.8921497,\n   0.8489351,\n   0.77299863,\n   0.91373914,\n   0.99233425,\n   0.98494565,\n   0.9893059,\n   0.96622247,\n   0.99766916,\n   0.11683264,\n   0.99774987,\n   0.99973696,\n   0.8749013,\n   0.98697424,\n   0.8977796,\n   0.35870916,\n   0.9998062,\n   0.9996557,\n   0.9956655,\n   0.6297297,\n   0.99964094,\n   0.99524397,\n   0.9974872,\n   0.99987245,\n   0.99986935,\n   0.9996922,\n   0.9995778,\n   0.9692666,\n   0.9986922,\n   0.9935342,\n   0.46925083,\n   0.84738356,\n   0.99611497,\n   0.99887174,\n   0.35135776,\n   0.42512655,\n   0.9425381,\n   0.48444527,\n   0.99985766,\n   0.35226214,\n   0.9990088,\n   0.8943272,\n   0.99917245,\n   0.99561286,\n   0.9904862,\n   0.9529528,\n   0.9987941,\n   0.06644642,\n   0.014063693,\n   0.0036654514,\n   0.0028276679,\n   0.015928112,\n   0.22320984,\n   0.98950255,\n   0.048630577,\n   0.026111856,\n   0.02758829,\n   0.102164224,\n   0.111582145,\n   0.0029164213,\n   0.06340191,\n   0.030239504,\n   0.39647266,\n   0.11858463,\n   0.01293249,\n   0.016603991,\n   0.049114197,\n   0.0016939549,\n   0.3170857,\n   0.9669982,\n   0.05802384,\n   0.018624412,\n   0.4492934,\n   0.020127725,\n   0.9925997,\n   0.021805208,\n   0.046989974,\n   0.006239858,\n   0.023004083,\n   0.32955784,\n   0.05066017,\n   0.02373808,\n   0.008297066,\n   0.12767561,\n   0.03168916,\n   0.009345974,\n   0.2675238,\n   0.063106135,\n   0.48726568,\n   0.092581205,\n   0.2334325,\n   0.10735449,\n   0.89641887,\n   0.091355376,\n   0.9977673,\n   0.0064271493,\n   0.10525409,\n   0.08707673,\n   0.12153138,\n   0.04285479,\n   0.05480351,\n   0.15363915,\n   0.9861435,\n   0.04698751,\n   0.047322493,\n   0.07586485,\n   0.029580409,\n   0.8678422,\n   0.99240464,\n   0.07950491,\n   0.99546933,\n   0.22778131,\n   0.08546987,\n   0.9991862,\n   0.9995065,\n   0.99901724,\n   0.99985254,\n   0.9990557,\n   0.99791926,\n   0.9997968,\n   0.99829847,\n   0.99842525,\n   0.08019796,\n   0.9995703,\n   0.99946445,\n   0.9984352,\n   0.18554015,\n   0.99990165,\n   0.99874145,\n   0.9994702,\n   0.99682355,\n   0.9991283,\n   0.9975254,\n   0.9990835,\n   0.998662,\n   0.9981793,\n   0.99882156,\n   0.9987503,\n   0.99318355,\n   0.9994791,\n   0.99942183,\n   0.9977209,\n   0.9839669,\n   0.999571,\n   0.9993622,\n   0.99799514,\n   0.9878243,\n   0.9197927,\n   0.99868387,\n   0.999902,\n   0.99895006,\n   0.9998907,\n   0.98906446,\n   0.9991136,\n   0.9895579,\n   0.089585654,\n   0.9977349,\n   0.9983215,\n   0.5014919,\n   0.9963199,\n   0.9994723,\n   0.9997651,\n   0.99883395,\n   0.9948107,\n   0.9986412,\n   0.09233422,\n   0.20081186,\n   0.9993537,\n   0.99930644,\n   0.9999101,\n   0.993638,\n   0.9985983,\n   0.9958682,\n   0.99983966,\n   0.9937344,\n   0.99906975,\n   0.99696153,\n   0.9957867,\n   0.9962962,\n   0.9997311,\n   0.9991934,\n   0.9997373,\n   0.99705255,\n   0.99804,\n   0.9989773,\n   0.9991873,\n   0.9943012,\n   0.9990637,\n   0.74297464,\n   0.998871,\n   0.99632925,\n   0.981155,\n   0.9985061,\n   0.99828184,\n   0.9624259,\n   0.99919766,\n   0.9997029,\n   0.9992004,\n   0.93517107,\n   0.99939334,\n   0.95591104,\n   0.9997328,\n   0.8727207,\n   0.9980379,\n   0.9974827,\n   0.99983597,\n   0.9998198,\n   0.99927205,\n   0.9989121,\n   0.9992661,\n   0.99968183,\n   0.99958056,\n   0.99826473,\n   0.9990681,\n   0.9990633,\n   0.013122803,\n   0.8480032,\n   0.99927837,\n   0.9986066,\n   0.7612364,\n   0.9230562,\n   0.9168527,\n   0.9992605,\n   0.32945183,\n   0.99900204,\n   0.9990829,\n   0.01198377,\n   0.9985695,\n   0.8666896,\n   0.9988489,\n   0.9986513,\n   0.48609483,\n   0.996009,\n   0.99806005,\n   0.98394495,\n   0.4037947,\n   0.999803,\n   0.9992725,\n   0.9995691,\n   0.99976844,\n   0.9937593,\n   0.9705995,\n   0.9988211,\n   0.99538124,\n   0.9987871,\n   0.9815714,\n   0.9992348,\n   0.999466,\n   0.9991659,\n   0.998243,\n   0.99886864,\n   0.5059464,\n   0.9984889,\n   0.99969673,\n   0.9994442,\n   0.9657172,\n   0.9982199,\n   0.9853507,\n   0.8127266,\n   0.9997285,\n   0.4428212,\n   0.998754,\n   0.9949203,\n   0.32836413,\n   0.99975175,\n   0.9959197,\n   0.99604404,\n   0.6364253,\n   0.99952745,\n   0.9997284,\n   0.9996026,\n   0.9999466,\n   0.9991616,\n   0.08052183,\n   0.99897814,\n   0.98988605,\n   0.99847656,\n   0.99571085,\n   0.99122435,\n   0.99253243,\n   0.99938035,\n   0.9998029,\n   0.8903708,\n   0.99875355,\n   0.9964451,\n   0.99935764,\n   0.99989295,\n   0.9994055,\n   0.99940324,\n   0.99927706,\n   0.9995546,\n   0.98695046,\n   0.99796164,\n   0.9985033,\n   0.9996879,\n   0.16847743,\n   0.9997763,\n   0.9991379,\n   0.9954809,\n   0.9994271,\n   0.99956006,\n   0.9995498,\n   0.99896526,\n   0.9993293,\n   0.97687304,\n   0.9064716,\n   0.9982797,\n   0.67945313,\n   0.997253,\n   0.99943453,\n   0.9990376,\n   0.99898225,\n   0.9897488,\n   0.9989536,\n   0.9995184,\n   0.978753,\n   0.99909186,\n   0.56821,\n   0.9996153,\n   0.48751336,\n   0.84327245,\n   0.99921596,\n   0.997617,\n   0.99691856,\n   0.84606254,\n   0.9978605,\n   0.99835616,\n   0.9977465,\n   0.99645895,\n   0.99952984,\n   0.9995029,\n   0.9991929,\n   0.9998814,\n   0.99897206,\n   0.83778113,\n   0.9993062,\n   0.99203426,\n   0.9997892,\n   0.8728633,\n   0.9474888,\n   0.5117668,\n   0.99907684,\n   0.99653447,\n   0.999385,\n   0.9992155,\n   0.9991167,\n   0.98552066,\n   0.9998832,\n   0.674453,\n   0.996792,\n   0.9787689,\n   0.3260934,\n   0.9966884,\n   0.99502194,\n   0.9999206,\n   0.9992286,\n   0.9573843,\n   0.9942058,\n   0.9981365,\n   0.99960726,\n   0.99935097,\n   0.99574536,\n   0.9986572,\n   0.993192,\n   0.99918705,\n   0.99901855,\n   0.9964503,\n   0.9995271,\n   0.9215672,\n   0.9995596,\n   0.99915075,\n   0.99919134,\n   0.9998473,\n   0.99821717,\n   0.7117655,\n   0.1484635,\n   0.9984964,\n   0.9978604,\n   0.9996369,\n   0.9982254,\n   0.99617815,\n   0.9989477,\n   0.90809566,\n   0.997471,\n   0.9994968,\n   0.99825746,\n   0.9996344,\n   0.9995528,\n   0.997792,\n   0.9986965,\n   0.99947304,\n   0.99853075,\n   0.9982516,\n   0.9992889,\n   0.9949767,\n   0.9997174,\n   0.9988945,\n   0.9994783,\n   0.9892051,\n   0.95230734,\n   0.9968516,\n   0.9994248,\n   0.82142514,\n   0.99684453,\n   0.9997675,\n   0.9997211,\n   0.9805269,\n   0.99734,\n   0.32409853,\n   0.82705134,\n   0.999915,\n   0.99897003,\n   0.006496469,\n   0.97991383,\n   0.9992699,\n   0.9995053,\n   0.99288803,\n   0.9950676,\n   0.9971916,\n   0.9991084,\n   0.8851288,\n   0.99946517,\n   0.99965405,\n   0.99418914,\n   0.124081336,\n   0.9996729,\n   0.98861957,\n   0.77804357,\n   0.9992841,\n   0.9992981,\n   0.9972088,\n   0.9979703,\n   0.99787724,\n   0.99820673,\n   0.97355574,\n   0.9009652,\n   0.9978091,\n   0.97886837,\n   0.99936193,\n   0.18758738,\n   0.9949256,\n   0.999666,\n   0.9997452,\n   0.99285567,\n   0.998836,\n   0.97055703,\n   0.80549026,\n   0.9997292,\n   0.9996679,\n   0.9993093,\n   0.9877988,\n   0.9966928,\n   0.15914823,\n   0.99973327,\n   0.9963649,\n   0.6748456,\n   0.43852043,\n   0.9993981,\n   0.9975211,\n   0.9984811,\n   0.9745681,\n   0.9992987,\n   0.9995061,\n   0.8973953,\n   0.9986304,\n   0.99784636,\n   0.99970835,\n   0.9989874,\n   0.9912445,\n   0.9803273,\n   0.9077504,\n   0.9990231,\n   0.9995053,\n   0.46279302,\n   0.9987128,\n   0.99968314,\n   0.86346906,\n   0.9998615,\n   0.9977011,\n   0.9986274,\n   0.9990013,\n   0.06469048,\n   0.6228152,\n   0.9989568,\n   0.9569687,\n   0.99639255,\n   0.9982899,\n   0.99871564,\n   0.9991025,\n   0.9977704,\n   0.9939558,\n   0.2819472,\n   0.9990355,\n   0.9976077,\n   0.9993875,\n   0.99950886,\n   0.9994444,\n   0.99935156,\n   0.925973,\n   0.9718422,\n   0.9994709,\n   0.99933726,\n   0.05691256,\n   0.9996741,\n   0.53612053,\n   0.99976736,\n   0.99719524,\n   0.22493751,\n   0.9998203,\n   0.99983096,\n   0.995507,\n   0.9995326,\n   0.9988751,\n   0.91726935,\n   0.30404514,\n   0.9973483,\n   0.9996841,\n   0.9939744,\n   0.8268336,\n   0.9975344,\n   0.99958426,\n   0.99597186,\n   0.9993175,\n   0.9797339,\n   0.9996817,\n   0.98799384,\n   0.8515803,\n   0.99892837,\n   0.99942917,\n   0.99875987,\n   0.28411832,\n   0.4281736,\n   0.99932015,\n   0.99948776,\n   0.99771214,\n   0.9995277,\n   0.9983156,\n   0.9042752,\n   0.99897504,\n   0.9971353,\n   0.99784255,\n   0.9993592,\n   0.9370308,\n   0.99958855,\n   0.9993231,\n   0.9920934,\n   0.99940133,\n   0.9659889,\n   0.9994343,\n   0.9988949,\n   0.99380964,\n   0.9928658,\n   0.07238567,\n   0.99943155,\n   0.9974854,\n   0.9996731,\n   0.9993855,\n   0.99981695,\n   0.9994646,\n   0.9992367,\n   0.99915993,\n   0.9991398,\n   0.97780776,\n   0.75674474,\n   0.9990139,\n   0.99783933,\n   0.11155208,\n   0.9993649,\n   0.08411097,\n   0.988796,\n   0.99863535,\n   0.9988675,\n   0.9988636,\n   0.2500902,\n   0.9998147,\n   0.9438351,\n   0.9508256,\n   0.99955255,\n   0.99917895,\n   0.97493935,\n   0.99977785,\n   0.99703,\n   0.9988894,\n   0.9469489,\n   0.99878377,\n   0.96767765,\n   0.9996908,\n   0.8510601,\n   0.9996275,\n   0.96690446,\n   0.99914694,\n   0.961291,\n   0.4982136,\n   0.9992322,\n   0.99799854,\n   0.12816863,\n   0.99731785,\n   0.9987702,\n   0.9979184,\n   0.88154984,\n   0.9990243,\n   0.9962525,\n   0.8749403,\n   0.99693525,\n   0.9959831,\n   0.99956506,\n   0.9989103,\n   0.99963593,\n   0.99500424,\n   0.9993345,\n   0.99961215,\n   0.99986327,\n   0.20293687,\n   0.9994665,\n   0.9981993,\n   0.9994355,\n   0.99890125,\n   0.9983083,\n   0.98590183,\n   0.999534,\n   0.99971217,\n   0.7241769,\n   0.31052858,\n   0.12989502,\n   0.99975973,\n   0.9973853,\n   0.9996216,\n   0.6846215,\n   0.9993315,\n   0.99922836,\n   0.9997292,\n   0.99984276,\n   0.9871912,\n   0.997726,\n   0.9993149,\n   0.9995803,\n   0.9985933,\n   0.8394869,\n   0.99972206,\n   0.97204036,\n   0.99867517,\n   0.99845695,\n   0.9955295,\n   0.99947923,\n   0.99483794,\n   0.99955577,\n   0.9976458,\n   0.99611545,\n   0.9996767,\n   0.99661094,\n   0.99835217,\n   0.6189936,\n   0.18159407,\n   0.9995696,\n   0.9995505,\n   0.99944574,\n   0.9998628,\n   0.9931958,\n   0.7178707,\n   0.8248969,\n   0.99903095,\n   0.99929,\n   0.9981066,\n   0.9785057,\n   0.9813875,\n   0.9966377,\n   0.99975353,\n   0.96394855,\n   0.55929166,\n   0.99813986,\n   0.99898845,\n   0.9995931,\n   0.06045644,\n   0.9958775,\n   0.2757397,\n   0.99245363,\n   0.9927468,\n   0.9993573,\n   0.99933213,\n   0.9838976,\n   0.9982272,\n   0.9983753,\n   0.9743522,\n   0.99979264,\n   0.9881175,\n   0.99873096,\n   0.03618638,\n   0.99954396,\n   0.9951519,\n   0.9998716,\n   0.27664825,\n   0.9992955,\n   0.9990683,\n   0.97326535,\n   0.9981071,\n   0.99875104,\n   0.9998074,\n   0.8623687,\n   0.96660364,\n   0.98502815,\n   0.99935037,\n   0.9991339,\n   0.9994185,\n   0.99909127,\n   0.9998282,\n   0.5704379,\n   0.99839264,\n   0.9891887,\n   0.9988682,\n   0.9993943,\n   0.99983,\n   0.9993793,\n   0.9991431,\n   0.9996643,\n   0.99957114,\n   0.9970139,\n   0.998273,\n   0.9131026,\n   0.992585,\n   0.99882823,\n   0.9996518,\n   0.99970907,\n   0.99589866,\n   0.9997888,\n   0.9991497,\n   0.9981674,\n   0.999438,\n   0.20929326,\n   0.9990959,\n   0.99848586,\n   0.9983924,\n   0.9961588,\n   0.99946135,\n   0.999501,\n   0.9951284,\n   0.97752404,\n   0.9995454,\n   0.1937658,\n   0.98723304,\n   0.99059045,\n   0.985781,\n   0.99849355,\n   0.9988487,\n   0.9991264,\n   0.99819356,\n   0.99977726,\n   0.9988288,\n   0.9999175,\n   0.43739808,\n   0.99801767,\n   0.9947454,\n   0.99918514,\n   0.9959668,\n   0.9992151,\n   0.9998938,\n   0.9989391,\n   0.99333364,\n   0.11616555,\n   0.13641761,\n   0.9993963,\n   0.21370427,\n   0.999508,\n   0.998063,\n   0.3115123,\n   0.9990221,\n   0.99980134,\n   0.9992391,\n   0.68573654,\n   0.99797136,\n   0.9994491,\n   0.98676664,\n   0.9952356,\n   0.99969685,\n   0.995877,\n   0.9970709,\n   0.9994419,\n   0.9959804,\n   0.99707437,\n   0.99932003,\n   0.9923645,\n   0.99946445,\n   0.7702994,\n   0.99893016,\n   0.99757725,\n   0.9509731,\n   0.13003103,\n   0.9990959,\n   0.9997656,\n   0.96210337,\n   0.99956673,\n   0.9990841,\n   0.99940383,\n   0.9739237,\n   0.9992661,\n   0.08574839,\n   0.99166733,\n   0.9996568,\n   0.9990031,\n   0.9985,\n   0.99985385,\n   0.9989473,\n   0.99591994,\n   0.99878377,\n   0.9992594,\n   0.9966724,\n   0.23117162,\n   0.99949884,\n   0.99938416,\n   0.99980074,\n   0.99659604,\n   0.99911505,\n   0.99846786,\n   0.9992337,\n   0.9937827,\n   0.99876726,\n   0.9994843,\n   0.9989925,\n   0.991981,\n   0.9996728,\n   0.9993339,\n   0.22025548,\n   0.9998932,\n   0.988277,\n   0.9940752,\n   0.9985915,\n   0.99985766,\n   0.10876735,\n   0.99967885,\n   0.0612331,\n   0.1889925,\n   0.8723563,\n   0.9995297,\n   0.99883014,\n   0.9993988,\n   0.999041,\n   0.9987942,\n   0.818927,\n   0.9997553,\n   0.9990821,\n   0.78313166,\n   0.9995546,\n   0.9994168,\n   0.9971065,\n   0.9997366,\n   0.9943257,\n   0.99953556,\n   0.9993247,\n   0.99485934,\n   0.75114447,\n   0.9980101,\n   0.9994529,\n   0.83867866,\n   0.09383044,\n   0.9985476,\n   0.9993766,\n   0.9986951,\n   0.99983215,\n   0.9989506,\n   0.9944746,\n   0.9997453,\n   0.99825376,\n   0.9998165,\n   0.999806,\n   0.99964094,\n   0.9987167,\n   0.9993593,\n   0.9990915,\n   0.9978582,\n   0.9966575,\n   0.9991749,\n   0.99921846,\n   0.9993086,\n   0.99833125,\n   0.29417273,\n   0.99946755,\n   0.999416,\n   0.9964768,\n   0.9950004,\n   0.8564292,\n   0.9910159,\n   0.9994666,\n   0.99501306,\n   0.7431337,\n   0.8969216,\n   0.9992884,\n   0.9990201,\n   0.9932294,\n   0.33743542,\n   0.99093294,\n   0.99943155,\n   0.9956077,\n   0.99505496,\n   0.99878925,\n   0.9933314,\n   0.9997768,\n   0.9948002,\n   0.9948447,\n   0.99595046,\n   0.97084683,\n   0.4250691,\n   0.99349797,\n   0.99763,\n   0.9992737,\n   0.9997429,\n   0.9965043,\n   0.99975747,\n   0.99910873,\n   0.99747974,\n   0.99604225,\n   0.43871516,\n   0.06920183,\n   0.19680077,\n   0.9972784,\n   0.99832577,\n   0.99954236,\n   0.99982554,\n   0.99881893,\n   0.70277125,\n   0.98929405,\n   0.99663323,\n   0.99826884,\n   0.977679,\n   0.9659457,\n   0.9670414,\n   0.9993469,\n   0.16361427,\n   0.0113374395,\n   0.9907849,\n   0.9743275,\n   0.9997435,\n   0.99740463,\n   0.99908423,\n   0.9989454,\n   0.9848647,\n   0.99960846,\n   0.9998273,\n   0.99977666,\n   0.9819385,\n   0.9923233,\n   0.99976546,\n   0.9986343,\n   0.02571444,\n   0.86329144,\n   0.3622346,\n   0.8835895,\n   0.998912,\n   0.9984497,\n   0.042471997,\n   0.9996264,\n   0.9996706,\n   0.9994106,\n   0.9479633,\n   0.9989895,\n   0.9993212,\n   0.113955095,\n   0.99861324,\n   0.034578793,\n   0.99929583,\n   0.9988757,\n   0.6385938,\n   0.9975115,\n   0.99991834,\n   0.9994783,\n   0.13813223,\n   0.99639755,\n   0.99596137,\n   0.9950321,\n   0.695463,\n   0.9890266,\n   0.9990823,\n   0.9995865,\n   0.99448144,\n   0.9993575,\n   0.9971692,\n   0.8418681,\n   0.99717605,\n   0.9993874,\n   0.996912,\n   0.9982691,\n   ...]),\n [tensor(0.2231, device='cuda:0'),\n  tensor(0.1792, device='cuda:0'),\n  tensor(0.1285, device='cuda:0'),\n  tensor(0.1684, device='cuda:0'),\n  tensor(0.3787, device='cuda:0'),\n  tensor(0.3758, device='cuda:0'),\n  tensor(0.4045, device='cuda:0'),\n  tensor(0.3760, device='cuda:0'),\n  tensor(0.2115, device='cuda:0'),\n  tensor(0.1886, device='cuda:0'),\n  tensor(0.1642, device='cuda:0'),\n  tensor(0.1784, device='cuda:0'),\n  tensor(0.3397, device='cuda:0'),\n  tensor(0.2840, device='cuda:0'),\n  tensor(0.2680, device='cuda:0'),\n  tensor(0.2609, device='cuda:0'),\n  tensor(0.0816, device='cuda:0'),\n  tensor(0.1750, device='cuda:0'),\n  tensor(0.2978, device='cuda:0'),\n  tensor(0.2364, device='cuda:0'),\n  tensor(0.2966, device='cuda:0'),\n  tensor(0.1318, device='cuda:0'),\n  tensor(0.2743, device='cuda:0'),\n  tensor(0.1706, device='cuda:0'),\n  tensor(0.3167, device='cuda:0'),\n  tensor(0.3829, device='cuda:0'),\n  tensor(0.2724, device='cuda:0'),\n  tensor(0.1105, device='cuda:0'),\n  tensor(0.1742, device='cuda:0'),\n  tensor(0.1999, device='cuda:0'),\n  tensor(0.1802, device='cuda:0'),\n  tensor(0.1922, device='cuda:0'),\n  tensor(0.1355, device='cuda:0'),\n  tensor(0.0969, device='cuda:0'),\n  tensor(0.1912, device='cuda:0'),\n  tensor(0.2056, device='cuda:0'),\n  tensor(0.1141, device='cuda:0'),\n  tensor(0.1457, device='cuda:0'),\n  tensor(0.1339, device='cuda:0'),\n  tensor(0.1588, device='cuda:0'),\n  tensor(0.2288, device='cuda:0'),\n  tensor(0.1569, device='cuda:0'),\n  tensor(0.1323, device='cuda:0'),\n  tensor(0.1256, device='cuda:0'),\n  tensor(0.0909, device='cuda:0'),\n  tensor(0.0647, device='cuda:0'),\n  tensor(0.1975, device='cuda:0'),\n  tensor(0.3255, device='cuda:0'),\n  tensor(0.2319, device='cuda:0'),\n  tensor(0.1945, device='cuda:0'),\n  tensor(0.2143, device='cuda:0'),\n  tensor(0.2133, device='cuda:0'),\n  tensor(0.3614, device='cuda:0'),\n  tensor(0.3076, device='cuda:0'),\n  tensor(0.3764, device='cuda:0'),\n  tensor(0.3978, device='cuda:0'),\n  tensor(0.3585, device='cuda:0'),\n  tensor(0.5669, device='cuda:0'),\n  tensor(0.5097, device='cuda:0'),\n  tensor(0.4532, device='cuda:0'),\n  tensor(0.4781, device='cuda:0'),\n  tensor(0.6037, device='cuda:0'),\n  tensor(0.4262, device='cuda:0'),\n  tensor(0.2140, device='cuda:0'),\n  tensor(0.2306, device='cuda:0'),\n  tensor(0.0703, device='cuda:0'),\n  tensor(0.4434, device='cuda:0'),\n  tensor(0.3142, device='cuda:0'),\n  tensor(0.3006, device='cuda:0'),\n  tensor(0.0943, device='cuda:0'),\n  tensor(0.1339, device='cuda:0'),\n  tensor(0.1375, device='cuda:0'),\n  tensor(0.1459, device='cuda:0'),\n  tensor(0.1232, device='cuda:0'),\n  tensor(0.1037, device='cuda:0'),\n  tensor(0.0871, device='cuda:0'),\n  tensor(0.0974, device='cuda:0'),\n  tensor(0.0773, device='cuda:0'),\n  tensor(0.1223, device='cuda:0'),\n  tensor(0.1108, device='cuda:0'),\n  tensor(0.1273, device='cuda:0'),\n  tensor(0.1301, device='cuda:0'),\n  tensor(0.0859, device='cuda:0'),\n  tensor(0.0972, device='cuda:0'),\n  tensor(0.0992, device='cuda:0'),\n  tensor(0.2326, device='cuda:0'),\n  tensor(0.0587, device='cuda:0'),\n  tensor(0.1418, device='cuda:0'),\n  tensor(0.1720, device='cuda:0'),\n  tensor(0.2314, device='cuda:0'),\n  tensor(0.2387, device='cuda:0'),\n  tensor(0.1563, device='cuda:0'),\n  tensor(0.1915, device='cuda:0'),\n  tensor(0.1711, device='cuda:0'),\n  tensor(0.2213, device='cuda:0'),\n  tensor(0.2544, device='cuda:0'),\n  tensor(0.4143, device='cuda:0'),\n  tensor(0.1741, device='cuda:0'),\n  tensor(0.2042, device='cuda:0'),\n  tensor(0.1694, device='cuda:0'),\n  tensor(0.2224, device='cuda:0'),\n  tensor(0.2239, device='cuda:0'),\n  tensor(0.1051, device='cuda:0'),\n  tensor(0.2379, device='cuda:0'),\n  tensor(0.1421, device='cuda:0'),\n  tensor(0.2003, device='cuda:0'),\n  tensor(0.1379, device='cuda:0'),\n  tensor(0.1793, device='cuda:0'),\n  tensor(0.2900, device='cuda:0'),\n  tensor(0.3044, device='cuda:0'),\n  tensor(0.1922, device='cuda:0'),\n  tensor(0.2061, device='cuda:0'),\n  tensor(0.2791, device='cuda:0'),\n  tensor(0.1384, device='cuda:0'),\n  tensor(0.2139, device='cuda:0'),\n  tensor(0.1962, device='cuda:0'),\n  tensor(0.2006, device='cuda:0'),\n  tensor(0.3081, device='cuda:0'),\n  tensor(0.1902, device='cuda:0'),\n  tensor(0.1903, device='cuda:0'),\n  tensor(0.2020, device='cuda:0'),\n  tensor(0.1361, device='cuda:0'),\n  tensor(0.1518, device='cuda:0'),\n  tensor(0.1127, device='cuda:0'),\n  tensor(0.1618, device='cuda:0'),\n  tensor(0.1664, device='cuda:0'),\n  tensor(0.1801, device='cuda:0'),\n  tensor(0.1266, device='cuda:0'),\n  tensor(0.3472, device='cuda:0'),\n  tensor(0.3124, device='cuda:0'),\n  tensor(0.2275, device='cuda:0'),\n  tensor(0.2193, device='cuda:0'),\n  tensor(0.1553, device='cuda:0'),\n  tensor(0.3226, device='cuda:0'),\n  tensor(0.2120, device='cuda:0'),\n  tensor(0.2318, device='cuda:0'),\n  tensor(0.2415, device='cuda:0'),\n  tensor(0.2446, device='cuda:0'),\n  tensor(0.2057, device='cuda:0'),\n  tensor(0.1925, device='cuda:0'),\n  tensor(0.3909, device='cuda:0'),\n  tensor(0.1214, device='cuda:0'),\n  tensor(0.2891, device='cuda:0'),\n  tensor(0.1579, device='cuda:0'),\n  tensor(0.2693, device='cuda:0'),\n  tensor(0.1481, device='cuda:0'),\n  tensor(0.1159, device='cuda:0'),\n  tensor(0.3412, device='cuda:0'),\n  tensor(0.2555, device='cuda:0'),\n  tensor(0.2767, device='cuda:0'),\n  tensor(0.2737, device='cuda:0')],\n (0.9705093165517205,\n  0.9150193785889277,\n  nan,\n  0.9162782382162514,\n  0.9273030397022333,\n  0.9026625757299982,\n  0.9055125066256089,\n  0.9273030397022333,\n  0.9688449665933705))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval_per_epoch(train_hla_pep_loader,None,None,criterion,1,0.5,True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:22:38.385204800Z",
     "start_time": "2023-10-18T14:21:25.105283300Z"
    }
   },
   "id": "c4f056a44251637c"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******开始验证******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:08<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是评估得分:\n",
      "MCC Error:  5395691622552000\n",
      "y_true: 0 = 8644 | 1 = 8500\n",
      "y_pred: 0 = 8366 | 1 = 8778\n",
      "tn = 8054, fp = 590, fn = 312, tp = 8188\n",
      "auc=0.9868|sensitivity=0.9633|specificity=0.9317|acc=0.9474|mcc=nan\n",
      "precision=0.9328|recall=0.9633|f1=0.9478|ap=0.9854\n",
      "******结束验证: Loss = 0.144655******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(([1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   0,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   1,\n   ...],\n  array([1, 1, 1, ..., 0, 0, 0]),\n  [0.8727261,\n   0.7199903,\n   0.9689057,\n   0.6248255,\n   0.92643696,\n   0.9780884,\n   0.40455937,\n   0.9083448,\n   0.9001241,\n   0.7675115,\n   0.87484056,\n   0.8989993,\n   0.98289156,\n   0.8136286,\n   0.9520157,\n   0.9005256,\n   0.02879604,\n   0.763434,\n   0.8744533,\n   0.97811675,\n   0.8599473,\n   0.80679256,\n   0.9735728,\n   0.88395154,\n   0.66603255,\n   0.85076666,\n   0.9872153,\n   0.7000948,\n   0.94665724,\n   0.5198574,\n   0.68315077,\n   0.6027637,\n   0.9678963,\n   0.9395277,\n   0.8293753,\n   0.5253283,\n   0.2723396,\n   0.962711,\n   0.15091786,\n   0.85806865,\n   0.5380528,\n   0.98995835,\n   0.8579285,\n   0.96506375,\n   0.95059294,\n   0.64193004,\n   0.87265885,\n   0.020918876,\n   0.69546574,\n   0.916193,\n   0.06546717,\n   0.79775304,\n   0.70427495,\n   0.24908586,\n   0.81081605,\n   0.8711194,\n   0.8248865,\n   0.86875284,\n   0.6644364,\n   0.97691476,\n   0.49094853,\n   0.9529824,\n   0.7418252,\n   0.9540276,\n   0.5343283,\n   0.68683404,\n   0.44041204,\n   0.6592218,\n   0.3856885,\n   0.094799645,\n   0.9306126,\n   0.6954569,\n   0.9383663,\n   0.3603803,\n   0.9606692,\n   0.91701275,\n   0.9779578,\n   0.99068224,\n   0.8063656,\n   0.61597633,\n   0.63561004,\n   0.70692676,\n   0.8718946,\n   0.85773325,\n   0.43792704,\n   0.78672034,\n   0.6990819,\n   0.76063275,\n   0.8809983,\n   0.96275926,\n   0.86331886,\n   0.84269035,\n   0.9410824,\n   0.9508995,\n   0.8770629,\n   0.94331884,\n   0.94275874,\n   0.9016565,\n   0.9549434,\n   0.97362024,\n   0.8703768,\n   0.9688428,\n   0.22626956,\n   0.85021454,\n   0.93504965,\n   0.9087412,\n   0.9175864,\n   0.8412335,\n   0.8672375,\n   0.8426771,\n   0.6887917,\n   0.74322975,\n   0.9614857,\n   0.7271343,\n   0.44805527,\n   0.99002075,\n   0.8865228,\n   0.8921703,\n   0.9753777,\n   0.60902166,\n   0.7632072,\n   0.93440396,\n   0.9417778,\n   0.96226645,\n   0.6767638,\n   0.14006309,\n   0.8773329,\n   0.95179576,\n   0.89828295,\n   0.94430417,\n   0.9298172,\n   0.9575305,\n   0.8636745,\n   0.7804035,\n   0.59389955,\n   0.86905074,\n   0.96115017,\n   0.8559128,\n   0.6270555,\n   0.30913287,\n   0.72643465,\n   0.43568176,\n   0.98244035,\n   0.59323967,\n   0.67145336,\n   0.9756805,\n   0.9591174,\n   0.89518505,\n   0.614411,\n   0.9318407,\n   0.8453601,\n   0.43367442,\n   0.10594826,\n   0.79190767,\n   0.22856702,\n   0.08722204,\n   0.057590563,\n   0.03472351,\n   0.49599665,\n   0.054674394,\n   0.8536806,\n   0.2689686,\n   0.44049624,\n   0.057567816,\n   0.07574769,\n   0.056505222,\n   0.22240005,\n   0.8787626,\n   0.01960579,\n   0.8041242,\n   0.17303497,\n   0.48323357,\n   0.10819954,\n   0.28690028,\n   0.64204544,\n   0.029530173,\n   0.0063973772,\n   0.11324273,\n   0.024016257,\n   0.021232788,\n   0.13805477,\n   0.077645265,\n   0.33706102,\n   0.19731738,\n   0.32352555,\n   0.17240399,\n   0.62340075,\n   0.26897627,\n   0.5340181,\n   0.023071663,\n   0.0572076,\n   0.43098402,\n   0.049430937,\n   0.9056389,\n   0.114895366,\n   0.6271656,\n   0.024803905,\n   0.09189497,\n   0.009941411,\n   0.009132781,\n   0.78440326,\n   0.16232456,\n   0.057135146,\n   0.16813622,\n   0.7621335,\n   0.0078689335,\n   0.0641924,\n   0.030916527,\n   0.020534737,\n   0.3204244,\n   0.050194465,\n   0.010914413,\n   0.03152206,\n   0.032092426,\n   0.011060701,\n   0.045115095,\n   0.38518283,\n   0.025385866,\n   0.45408028,\n   0.023794578,\n   0.022317648,\n   0.12635443,\n   0.12256873,\n   0.22274885,\n   0.02844789,\n   0.24340089,\n   0.08800257,\n   0.34736836,\n   0.19121319,\n   0.044755094,\n   0.017876027,\n   0.35765958,\n   0.5800657,\n   0.5798068,\n   0.17096826,\n   0.08982358,\n   0.017252484,\n   0.06535338,\n   0.01684245,\n   0.008371451,\n   0.010156677,\n   0.5490229,\n   0.057939116,\n   0.13611552,\n   0.26075664,\n   0.015216261,\n   0.042414553,\n   0.3127482,\n   0.19213757,\n   0.53969,\n   0.16972618,\n   0.11159423,\n   0.014696901,\n   0.1562969,\n   0.069931515,\n   0.53541577,\n   0.7115812,\n   0.23453341,\n   0.10073037,\n   0.05604348,\n   0.0236964,\n   0.07768217,\n   0.014270279,\n   0.7234314,\n   0.14832367,\n   0.13117342,\n   0.33440983,\n   0.54651874,\n   0.23014458,\n   0.7118949,\n   0.14831433,\n   0.36002207,\n   0.49217853,\n   0.018511893,\n   0.20556837,\n   0.17800955,\n   0.45717153,\n   0.034136675,\n   0.19498204,\n   0.025763215,\n   0.7587813,\n   0.19987726,\n   0.19003163,\n   0.6720211,\n   0.062377427,\n   0.18919392,\n   0.13762371,\n   0.18582128,\n   0.025143182,\n   0.03798617,\n   0.10868909,\n   0.4206656,\n   0.0076803467,\n   0.03856018,\n   0.034391217,\n   0.2626452,\n   0.014567512,\n   0.21089604,\n   0.8183395,\n   0.2235325,\n   0.13886313,\n   0.048803896,\n   0.8930801,\n   0.7484407,\n   0.14830601,\n   0.059057467,\n   0.41644624,\n   0.011973768,\n   0.6793858,\n   0.031433605,\n   0.025885966,\n   0.6000694,\n   0.04241686,\n   0.19993182,\n   0.21127957,\n   0.10934782,\n   0.43424302,\n   0.23793253,\n   0.10665413,\n   0.006184412,\n   0.048340082,\n   0.08481817,\n   0.052215964,\n   0.46941793,\n   0.3566262,\n   0.040114153,\n   0.097672656,\n   0.08998857,\n   0.0389366,\n   0.38034663,\n   0.7251415,\n   0.86009705,\n   0.2325234,\n   0.06993102,\n   0.07614982,\n   0.01921621,\n   0.5326638,\n   0.05049414,\n   0.02137163,\n   0.086349085,\n   0.22532156,\n   0.9284639,\n   0.03252612,\n   0.0093374215,\n   0.25509518,\n   0.063355,\n   0.07449438,\n   0.020198587,\n   0.60036135,\n   0.26910222,\n   0.19083166,\n   0.025356729,\n   0.09289559,\n   0.009875291,\n   0.039519086,\n   0.08961685,\n   0.06966766,\n   0.4934379,\n   0.24671634,\n   0.20021273,\n   0.19098349,\n   0.39052975,\n   0.16159071,\n   0.18123125,\n   0.20958842,\n   0.08190231,\n   0.055775337,\n   0.00726341,\n   0.026415853,\n   0.861215,\n   0.7279052,\n   0.090790994,\n   0.34338567,\n   0.016610958,\n   0.5576465,\n   0.20990388,\n   0.19837137,\n   0.35633397,\n   0.6128431,\n   0.006460652,\n   0.101377346,\n   0.0072742007,\n   0.0022713535,\n   0.05742282,\n   0.1737164,\n   0.03430512,\n   0.269008,\n   0.023494132,\n   0.13008632,\n   0.04308433,\n   0.055337198,\n   0.57171476,\n   0.049795624,\n   0.009849471,\n   0.56745255,\n   0.087202355,\n   0.19128329,\n   0.107350536,\n   0.03548595,\n   0.6530102,\n   0.53393304,\n   0.011692184,\n   0.20827502,\n   0.03232346,\n   0.08218792,\n   0.20698446,\n   0.8440809,\n   0.011902757,\n   0.2633959,\n   0.51186424,\n   0.86540174,\n   0.28453532,\n   0.1938437,\n   0.101441726,\n   0.36788052,\n   0.118858755,\n   0.12839459,\n   0.040213365,\n   0.0064713736,\n   0.4724659,\n   0.51110625,\n   0.4578784,\n   0.23687762,\n   0.5988554,\n   0.049991895,\n   0.09513746,\n   0.042251863,\n   0.11564365,\n   0.06196241,\n   0.06996513,\n   0.12457418,\n   0.010763609,\n   0.20067678,\n   0.14313468,\n   0.0627894,\n   0.0061416104,\n   0.08179986,\n   0.054700926,\n   0.49564257,\n   0.03137184,\n   0.6665315,\n   0.7209031,\n   0.51863635,\n   0.032727137,\n   0.49024627,\n   0.76470083,\n   0.0071722763,\n   0.15420766,\n   0.040431526,\n   0.21836247,\n   0.097471915,\n   0.6099351,\n   0.05806289,\n   0.15769832,\n   0.008706454,\n   0.9740424,\n   0.9603005,\n   0.7616169,\n   0.99108,\n   0.8433721,\n   0.93794453,\n   0.7984728,\n   0.96527404,\n   0.77414775,\n   0.9444283,\n   0.0796888,\n   0.9320487,\n   0.90078044,\n   0.91922706,\n   0.9037592,\n   0.47087625,\n   0.9860027,\n   0.67670405,\n   0.632086,\n   0.89305884,\n   0.7986397,\n   0.98331845,\n   0.73920333,\n   0.7336056,\n   0.09399357,\n   0.8987288,\n   0.96756685,\n   0.8900054,\n   0.6937742,\n   0.98483837,\n   0.8233138,\n   0.9426887,\n   0.98206896,\n   0.94617975,\n   0.24021068,\n   0.9204758,\n   0.907885,\n   0.6942768,\n   0.98070025,\n   0.9559438,\n   0.9362363,\n   0.93691534,\n   0.9928203,\n   0.9203854,\n   0.9351838,\n   0.3647268,\n   0.9658661,\n   0.97352785,\n   0.9141027,\n   0.9393588,\n   0.9738554,\n   0.88881296,\n   0.86810035,\n   0.92326117,\n   0.8216221,\n   0.8544054,\n   0.97087395,\n   0.7485132,\n   0.90362936,\n   0.9170539,\n   0.9712487,\n   0.17714709,\n   0.84744877,\n   0.35656774,\n   0.032702368,\n   0.94005466,\n   0.5995776,\n   0.03925047,\n   0.29815373,\n   0.6980326,\n   0.06308114,\n   0.08925883,\n   0.5505461,\n   0.2392891,\n   0.14169179,\n   0.3952187,\n   0.1846231,\n   0.09150012,\n   0.26143652,\n   0.029743796,\n   0.09953801,\n   0.6129844,\n   0.07821089,\n   0.063390456,\n   0.023685602,\n   0.18360503,\n   0.42844954,\n   0.11818764,\n   0.42754644,\n   0.026101,\n   0.33996028,\n   0.7284078,\n   0.03241549,\n   0.34578028,\n   0.17059971,\n   0.30724287,\n   0.11145627,\n   0.047740467,\n   0.28662303,\n   0.079506174,\n   0.06221607,\n   0.019898439,\n   0.531289,\n   0.16558728,\n   0.17829491,\n   0.617508,\n   0.14650653,\n   0.7517135,\n   0.29316017,\n   0.6214717,\n   0.10292832,\n   0.4172631,\n   0.12844785,\n   0.027189765,\n   0.33351412,\n   0.06567731,\n   0.3416115,\n   0.04738972,\n   0.6195852,\n   0.05286342,\n   0.8067028,\n   0.015586281,\n   0.019251429,\n   0.36001793,\n   0.2531176,\n   0.041928474,\n   0.46729922,\n   0.029498542,\n   0.0054979986,\n   0.7176975,\n   0.05401399,\n   0.48810643,\n   0.12523389,\n   0.05428915,\n   0.03603923,\n   0.08879155,\n   0.05827658,\n   0.10794221,\n   0.92393833,\n   0.019121941,\n   0.97427034,\n   0.7619368,\n   0.9412992,\n   0.11750401,\n   0.5205329,\n   0.8450135,\n   0.95755696,\n   0.42517045,\n   0.96713656,\n   0.77316695,\n   0.94500095,\n   0.6040573,\n   0.82988125,\n   0.9080969,\n   0.9418216,\n   0.47472373,\n   0.9735631,\n   0.95595485,\n   0.9527898,\n   0.943916,\n   0.83030343,\n   0.94625014,\n   0.9212381,\n   0.9735223,\n   0.9640786,\n   0.92213696,\n   0.94117814,\n   0.962236,\n   0.89141524,\n   0.9329252,\n   0.85604715,\n   0.09578288,\n   0.19388449,\n   0.942841,\n   0.58028245,\n   0.9830892,\n   0.8997231,\n   0.92097795,\n   0.74929905,\n   0.5335568,\n   0.535779,\n   0.043567117,\n   0.37731132,\n   0.2638898,\n   0.2427654,\n   0.04401921,\n   0.21457052,\n   0.081483595,\n   0.74847364,\n   0.4035582,\n   0.13300078,\n   0.079553925,\n   0.8185865,\n   0.0988583,\n   0.5047449,\n   0.18398589,\n   0.043041375,\n   0.26860762,\n   0.16097665,\n   0.908298,\n   0.8277124,\n   0.9632209,\n   0.3536615,\n   0.22754666,\n   0.09744115,\n   0.64760643,\n   0.06250179,\n   0.11135415,\n   0.018549453,\n   0.11111532,\n   0.97314453,\n   0.032104425,\n   0.18309279,\n   0.2982134,\n   0.04498164,\n   0.12661435,\n   0.12574436,\n   0.2080244,\n   0.8449363,\n   0.8258417,\n   0.8464605,\n   0.9392866,\n   0.19708282,\n   0.62125844,\n   0.97918177,\n   0.98737806,\n   0.8970718,\n   0.68801117,\n   0.8542362,\n   0.7679293,\n   0.7707735,\n   0.6290208,\n   0.07705787,\n   0.95746636,\n   0.72522306,\n   0.58605456,\n   0.978226,\n   0.24535662,\n   0.95064765,\n   0.12328549,\n   0.16977051,\n   0.14985295,\n   0.088754945,\n   0.8186164,\n   0.37981188,\n   0.8333793,\n   0.27006814,\n   0.14560834,\n   0.75042784,\n   0.5880875,\n   0.20665802,\n   0.5714627,\n   0.20505184,\n   0.72781104,\n   0.25074434,\n   0.1830846,\n   0.551594,\n   0.9095012,\n   0.7841667,\n   0.19232868,\n   0.9200211,\n   0.8041281,\n   0.6981128,\n   0.8100004,\n   0.88740534,\n   0.88307506,\n   0.9555576,\n   0.9850761,\n   0.989035,\n   0.8872121,\n   0.9846623,\n   0.97174567,\n   0.988304,\n   0.9154607,\n   0.2565455,\n   0.96110326,\n   0.87957305,\n   0.4913724,\n   0.8316884,\n   0.97200453,\n   0.3521614,\n   0.40411124,\n   0.24048539,\n   0.04702541,\n   0.5521809,\n   0.8918961,\n   0.74757916,\n   0.70187575,\n   0.4264639,\n   0.97607064,\n   0.25537145,\n   0.49828628,\n   0.9726619,\n   0.15605839,\n   0.65384734,\n   0.44467345,\n   0.41376346,\n   0.1263115,\n   0.20788181,\n   0.9798559,\n   0.11467589,\n   0.8484122,\n   0.96992606,\n   0.8806689,\n   0.8923892,\n   0.8690153,\n   0.74084026,\n   0.94108367,\n   0.9441215,\n   0.9926408,\n   0.49274907,\n   0.8919381,\n   0.5939435,\n   0.10195667,\n   0.9817872,\n   0.05630803,\n   0.0671544,\n   0.04636178,\n   0.0075962045,\n   0.5746562,\n   0.09147642,\n   0.005096488,\n   0.036197506,\n   0.0019683826,\n   0.051191643,\n   0.24790333,\n   0.011241127,\n   0.48992276,\n   0.5788463,\n   0.025286976,\n   0.0022769906,\n   0.03670924,\n   0.4383138,\n   0.90077,\n   0.95325243,\n   0.39642566,\n   0.9865802,\n   0.9000948,\n   0.8636057,\n   0.49679333,\n   0.4617693,\n   0.5790687,\n   0.9346249,\n   0.9914792,\n   0.6037822,\n   0.9250545,\n   0.93889016,\n   0.61603767,\n   0.39559504,\n   0.40565377,\n   0.5121742,\n   0.8553303,\n   0.8362675,\n   0.8831136,\n   0.98487246,\n   0.76024556,\n   0.96581227,\n   0.9830345,\n   0.9228085,\n   0.7497577,\n   0.93051165,\n   0.98759437,\n   0.4573957,\n   0.8166258,\n   0.9272071,\n   0.49767312,\n   0.87928075,\n   0.9288711,\n   0.980445,\n   0.85236627,\n   0.9666745,\n   0.051856156,\n   0.8668068,\n   0.975002,\n   0.67274356,\n   0.8494128,\n   0.8162448,\n   0.930019,\n   0.8272148,\n   0.8906493,\n   0.52850395,\n   0.13092913,\n   0.913164,\n   0.8190874,\n   0.95285827,\n   0.7827981,\n   0.98948365,\n   0.8929174,\n   0.9655024,\n   0.800532,\n   0.6526691,\n   0.9341131,\n   0.8008462,\n   0.7901635,\n   0.72385293,\n   0.9835992,\n   0.941385,\n   0.95717764,\n   0.9353141,\n   0.96146953,\n   0.11261067,\n   0.96709645,\n   0.9501008,\n   0.87606025,\n   0.9111573,\n   0.98342395,\n   0.4373975,\n   0.5725393,\n   0.95849794,\n   0.9228558,\n   0.844145,\n   0.93550396,\n   0.97610044,\n   0.8855193,\n   0.60656184,\n   0.76094216,\n   0.9411843,\n   0.9063425,\n   0.7056897,\n   0.91011673,\n   0.7976257,\n   0.86935383,\n   0.9691131,\n   0.88280666,\n   0.60188687,\n   0.96240985,\n   0.97250193,\n   0.87449646,\n   0.9801743,\n   0.05434761,\n   0.7571897,\n   0.89777887,\n   0.86716795,\n   0.85837173,\n   0.28266364,\n   0.90380603,\n   0.8834736,\n   0.89515334,\n   0.9012846,\n   0.94712836,\n   0.9818479,\n   0.56167465,\n   0.8362131,\n   0.9620687,\n   0.93584627,\n   0.9730214,\n   0.9202038,\n   0.6566999,\n   0.979679,\n   0.95727223,\n   0.9598606,\n   0.55876106,\n   0.9456241,\n   0.35538784,\n   0.9664532,\n   0.9392884,\n   0.6817537,\n   0.76601934,\n   0.98880357,\n   0.90387434,\n   0.9874906,\n   0.931304,\n   0.91730905,\n   0.96437263,\n   0.9850429,\n   0.62095565,\n   0.992969,\n   0.92021406,\n   0.6215039,\n   0.97164965,\n   0.9752195,\n   0.38954237,\n   0.79362214,\n   0.9561625,\n   0.9532678,\n   0.9252685,\n   0.80158836,\n   0.57840174,\n   0.97710514,\n   0.65352815,\n   0.8790896,\n   0.99147075,\n   0.92398995,\n   0.8513507,\n   0.947874,\n   0.9794135,\n   0.5365578,\n   0.97878325,\n   0.98186594,\n   0.9861632,\n   0.9134132,\n   0.90853363,\n   0.8692626,\n   0.9684766,\n   0.97843164,\n   0.79844517,\n   0.85022557,\n   0.9374176,\n   0.9696973,\n   0.6137298,\n   0.99454015,\n   0.26472822,\n   0.96661544,\n   0.8298682,\n   0.95108753,\n   0.98937196,\n   0.2729485,\n   0.91339445,\n   0.9741002,\n   0.9548856,\n   0.6990814,\n   0.9636528,\n   0.96770245,\n   0.94195473,\n   0.8920109,\n   0.98474807,\n   0.791884,\n   0.9792322,\n   0.99025327,\n   0.94114363,\n   0.9691759,\n   0.57419366,\n   0.9908167,\n   0.86467355,\n   0.9287867,\n   0.9837878,\n   0.9847258,\n   0.659656,\n   0.8858001,\n   0.88474154,\n   0.9376632,\n   0.95959747,\n   0.44900453,\n   0.46570563,\n   0.91050935,\n   0.82518595,\n   0.93667054,\n   0.99095047,\n   0.67661697,\n   0.91901743,\n   0.93512887,\n   0.94142354,\n   0.75541663,\n   0.8867711,\n   0.941833,\n   ...]),\n [tensor(0.3606, device='cuda:0'),\n  tensor(0.2278, device='cuda:0'),\n  tensor(0.2020, device='cuda:0'),\n  tensor(0.0762, device='cuda:0'),\n  tensor(0.0782, device='cuda:0'),\n  tensor(0.1034, device='cuda:0'),\n  tensor(0.1113, device='cuda:0'),\n  tensor(0.0802, device='cuda:0'),\n  tensor(0.0862, device='cuda:0'),\n  tensor(0.0637, device='cuda:0'),\n  tensor(0.1742, device='cuda:0'),\n  tensor(0.2066, device='cuda:0'),\n  tensor(0.0693, device='cuda:0'),\n  tensor(0.1120, device='cuda:0'),\n  tensor(0.1861, device='cuda:0'),\n  tensor(0.1265, device='cuda:0'),\n  tensor(0.1947, device='cuda:0')],\n (0.9868326618940033,\n  0.9473868408772749,\n  nan,\n  0.9477948836670912,\n  0.9632941176470589,\n  0.9317445627024525,\n  0.9327865117338802,\n  0.9632941176470589,\n  0.9854453696894968))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval_per_epoch(test_hla_pep_loader,None,None,criterion,1,0.5,True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:23:39.001349700Z",
     "start_time": "2023-10-18T14:23:30.331122300Z"
    }
   },
   "id": "b1fd2a5dfff297d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "86249026da37768"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
